{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfc3a0d-777c-4e95-b242-8c130d51bd3d",
   "metadata": {},
   "source": [
    "# **Classification**\n",
    "\n",
    "Problems that need a *quantitative* response (numeric value) are **regression** ; problems that need a *qualitative* response (boolean or category) are **classification**. Many statistical methods can be applied to both types of problems.\n",
    "\n",
    "**Binary** classification have two output classes. They usually end up being \"A\" and \"not A\". Examples are \"earthquake\" or \"no earthquake=noise\". **Multiclass** classification refers to one with more than two classes.\n",
    "\n",
    "Classification here requires that we know the labeled, it is a form of *supervised learning*.\n",
    "\n",
    "There are several classifier algorithms, which we will summarize below before practicing.\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)**: The LDA optimiziation methods produces an optimal dimensionality reductions to a decision line for classificaiton. It is based on variance reduction and has analogy to a PCA coordinate system.\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**:\n",
    "\n",
    "* **Naive Bayes (NB)**: Simple algorithm that requires little hyper-parameters, provides interpretable results. The algorithm computes conditional probabilities and uses the product as a decision rule to maximize the probability in each class.\n",
    "\n",
    "* **K-nearest neighbors (KNN)**: Choose *K* as the numbers of nearest data points to consider. Gather each data sample and the K nearest ones, assign the class that is most represented in that group (the mode of the K labels).\n",
    "\n",
    "* **Support Vector Machine (SVM)**: Finds the hyperplanes that separate the classes with sufficient margins. The hyperplanes can be linear and more complex (kernels SVM such as radial basis function and polynomial kernels). SVM was very popular for limited training data.\n",
    "\n",
    "* **Random Forest (RF)**: Decision trees are common for prediction pipelines. *Decision tree learning* is a method to create a predictive model of trees based on the data. More on that this monday.\n",
    "\n",
    " Some classifiers can handle multi class nateively (Stochastic Gradient Descent - SGD; Random Forest classification;  Naive Bayes). Others are strictly binary classifiers (Logistic Regression, Support Vector Machine classifier - SVM). \n",
    "\n",
    " We can compare them all in one [exercise](!https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6107cdf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'naive' from 'sklearn.neighbors' (/srv/conda/envs/notebook/lib/python3.8/site-packages/sklearn/neighbors/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_moons, make_circles, make_classification\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# classifiers from sklearns\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m naive\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgaussian_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianProcessClassifier\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'naive' from 'sklearn.neighbors' (/srv/conda/envs/notebook/lib/python3.8/site-packages/sklearn/neighbors/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# basic tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# classifiers from sklearns\n",
    "from sklearn.neighbors import naive\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    naive(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 3 types of data sets\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1d400-b37b-4fa8-a813-2bfe15f655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d19813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the first data sets, and use sklearn to classify\n",
    "\n",
    "\n",
    "# preprocess dataset, split into training and test part\n",
    "X, y = datasets[0]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "# random labels\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN play with K\n",
    "K=12\n",
    "clf= KNeighborsClassifier(K)\n",
    "#clf = classifiers[0]\n",
    "print(clf)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430fb6b-1c2d-4c20-b855-5aaca5f1e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "clf=classifiers[5]\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a19f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1 + 1, 1)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "# Plot the testing points\n",
    "ax.scatter(\n",
    "    X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    ")\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "ax.scatter(\n",
    "    X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    ")\n",
    "# Plot the testing points\n",
    "ax.scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    c=y_test,\n",
    "    cmap=cm_bright,\n",
    "    edgecolors=\"k\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed4b70-5e28-45c1-8bbc-d7926603d142",
   "metadata": {},
   "source": [
    "## Classifier Performance Metrics\n",
    "\n",
    "In a binary classifier, we label one of the two classes as *positive*, the other class is negative. Let's consider *N* data samples.\n",
    "\n",
    "| True Class      | Positive            | Negative           | Total |\n",
    "|  -------------  |  -----------------  |  --------------- | ----- |\n",
    "| Positive        | True positive   | False negative | p     |\n",
    "| Negative        | False positive  | True negative  | n     |\n",
    "| **Total**       | p'                  | n'                 | N     |\n",
    "\n",
    "**True positive (TP)**: the label is predicted as *positive* and it is *correct/true*.\n",
    "\n",
    "**True negative (TN)**: the label is predicted as *negative* (i.e., the other class) and it is *correct/true*.\n",
    "\n",
    "**False positive (FP)**: the label is predicted as *positive* and it is *incorrect/false*, it should have been the other class.\n",
    "\n",
    "**False negative (FN)**: the label is predicted as *negative* and it is *incorrect/false*, it should have been the other class.\n",
    "\n",
    "\n",
    "**Confusion matrix:**\n",
    "Count the instances that an element of class *A* is classified in class *B*. A 2-class confusion matrix is:\n",
    "\n",
    "$ C = \\begin{array}{|cc|} TP & FN \\\\ FP  & TN \\end{array}$\n",
    "\n",
    "The confusion matrix can be extended for a multi-class classification and the matrix is KxK instead of 2x2. The best confusion matrix is one that is close to identity, with little off diagnoal terms.\n",
    "\n",
    "**Other model performance metics**\n",
    "Model peformance can be assessed wih the following:\n",
    "* **error** : the proportion of the data that was misclassified \n",
    "\n",
    "    $err = \\frac{FP+FN}{N}$  -> 0\n",
    "* **accuracy**: the proportion of the data that was correctly classified: \n",
    "    \n",
    "    $acc = \\frac{TP + TN}{N} = 1 - err $ --> 1\n",
    "* **TP-rate**: the ratio of samples correctly class in the *positive* class:\n",
    "\n",
    "    $TPR = \\frac{TP}{P}$ --> 1\n",
    "* **FP-rate**: the ratio of samples that were correctly classified in the *negative* class:\n",
    "\n",
    "    $FPR = \\frac{fp}{n}$ --> 0\n",
    "* **precision**: the fraction of correctly classified in the first/positive class:\n",
    "\n",
    "    $Pr = \\frac{TP}{TP+FP}$ --> 1\n",
    "* **recall** or **sensitivity**: fraction of positive samples.\n",
    "\n",
    "    $Pr = \\frac{TP}{TP+FN}$ --> 1\n",
    "\n",
    "* **specificity**: fraction of well retrieved negative samples.\n",
    "\n",
    "    $Sp = \\frac{TN}{FP+FN}$ --> 1\n",
    "\n",
    "* **F1 score**:\n",
    "\n",
    "    $F_1 = \\frac{2}{(1/ precision + 1/recall)} = \\frac{TP}{TP + (FN+FP)/2} $ --> 1.\n",
    "\n",
    "    \n",
    "The harmonic mean of the F1 scores gives more weight to low values. F1 score is thus high if both recall and precision are high.\n",
    "\n",
    "A good way to evaluate a model is also to use cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d665ece",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Data download\n",
    "\n",
    "We will practice with the MNIST data set. It is a data sets of images of handwritten numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751b154-6566-427a-bb55-824c3111e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits,fetch_openml\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148193a-5847-4ce8-9a79-898a56f68ffb",
   "metadata": {},
   "source": [
    "The data is vector of floats. The target is an integer that is the attribute of the data. How are the data balanced between the classes? How many samples are there per class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bf0df-9471-40ec-a5d3-33e4b84bf915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data type\n",
    "data,y = digits[\"data\"].copy(),digits[\"target\"].copy()\n",
    "print(type(data[0][:]),type(y[0]))\n",
    "print(data[0][:])\n",
    "print(y[0])\n",
    "print(max(data[0]))\n",
    "# note that we do not modify the raw data that is stored on the digits dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eea66f-d3bf-48f6-aa3e-5d7c0b92586e",
   "metadata": {},
   "source": [
    " **how many classes are there?**\n",
    " Since the classes are integers, we can count the number of classes using the function \"unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1ecff-34cf-4cce-bf99-3787d51b466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nclasses = len(np.unique(y))\n",
    "print(np.unique(y))\n",
    "print(Nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e39ff-6ba5-44e3-909c-0112f8c9f1cd",
   "metadata": {},
   "source": [
    "### 1.1 Data preparation\n",
    "First print and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f78a0-23e9-41ee-8ed6-b58a2b557129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "# plot the first 4 data and their labels.\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f80dd-f3b9-4d7b-a5dd-b7d581947017",
   "metadata": {},
   "source": [
    "We look at it and there is little noise and no gap. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34c283-6190-417b-afae-13516867f8f6",
   "metadata": {},
   "source": [
    "### 1.2 Data re-scaling\n",
    "\n",
    "We could use MinMaxScaler from ``sklearn.preprocessing`` but since the formula for that is (x-min)/(max-min) and our min is 0, we could directly calculate x/max.\n",
    "(notes from https://www.kaggle.com/recepinanc/mnist-classification-sklearn)\n",
    "Note that the raw data is still stored in the dictionary ``digits`` and so we can modify the ``data`` variable in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d086c50-a54e-488d-8b33-b939e34ab8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(data[0]),max(data[0]))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data)# fit the model for data normalization\n",
    "newdata = scaler.transform(data) # transform the data. watch that data was converted to a numpy array\n",
    "print(type(newdata))\n",
    "print(newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b2c0b-8eca-46ce-864e-a5b41aa4bf4f",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c983d-97ec-4554-ba9f-718ff48b5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 50% train and 50% test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(f\"There are {data.shape[0]} data samples\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.5, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744f64e-81dc-433a-9f30-b9254d3a3248",
   "metadata": {},
   "source": [
    "## **Binary Classification**\n",
    "\n",
    "We will first attempt to identify two classes: \"5\" and \"not 5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3e700-d253-4668-b956-bfff6883032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train==5)\n",
    "y_test_5 = (y_test==5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95d695-e065-4fca-94b9-2d1a8e6322ce",
   "metadata": {},
   "source": [
    "We will first use a classic classifier: ***Stochastic Gradient Descent SGD***.  To reproduce the results, you should set the random_state parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf74217-e876-4e17-ac66-6308e7ffbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train,y_train_5)\n",
    "# test on the first element of the data sample and its label:\n",
    "print(data[0])\n",
    "print(y[0])\n",
    "print(\"Prediction of the first data '%1.0f' onto whether it belongs to the class 5 is %s.\" %(y[0],sgd_clf.predict([data[0]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66bc09-47be-44bd-997f-436a647fc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf,X_train,y_train_5,cv=3) # predict using K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3513b-ec13-4723-9473-6493ab3dce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score \n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(y_train_5,y_train_pred))\n",
    "print(\"precison, recall\")\n",
    "print(precision_score(y_train_5,y_train_pred),recall_score(y_train_5,y_train_pred))\n",
    "print(\"F1 score\")\n",
    "print(f1_score(y_train_5,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e4b8b-822a-41f8-a071-f97164ad6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(f\"Classification report for classifier {sgd_clf}:\\n\"\n",
    "      f\"{metrics.classification_report(y_train_5, y_train_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441a469-3504-4d82-ab04-10f6f7e767ef",
   "metadata": {},
   "source": [
    "**Precision and recall trade off**: increasing precision reduces recall. The classifier uses a *threshold* value to decide whether a data belongs to a class. Increasing the threhold gives higher precision score, decreasing the thresholds gives higher recall scores. Let's look at the various score values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6db181-f52d-423d-8137-f610c2305226",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score=sgd_clf.decision_function([data[0]])\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013ccee-4a79-4431-a8b1-3726a1e63208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "y_score=cross_val_predict(sgd_clf,X_train,y_train_5,cv=4,method=\"decision_function\")\n",
    "precisions,recalls,thresholds=precision_recall_curve(y_train_5,y_score)\n",
    "plt.plot(thresholds,precisions[:-1])\n",
    "plt.plot(thresholds,recalls[:-1],'g-')\n",
    "plt.legend(['Precision','Recall'])\n",
    "plt.grid(True)\n",
    "plt.xlabel('Score thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9aa35-38d2-46a5-8be8-0265a906bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recalls[:-1],precisions[:-1])\n",
    "plt.grid(True)\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc91f31-6f28-48d7-8810-6438651f5811",
   "metadata": {},
   "source": [
    "Given the tradeoff, we can now choose a specific threshold to tune your classification. It seems that the precision drops below 90% when the recall value gets above 90% as well. So we can choose the threshold of 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958b3aa-d9ea-4396-b344-2a17db779bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision=thresholds[np.argmax(precisions>=0.9)]\n",
    "y_train_pred_90 = (y_score >=threshold_90_precision)\n",
    "\n",
    "print(precision_score(y_train_5,y_train_pred_90))\n",
    "print(recall_score(y_train_5,y_train_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ad974-bf14-4d70-8b38-6ac123e53718",
   "metadata": {},
   "source": [
    "**Receiver Operating Characteristics ROC** \n",
    "\n",
    "It plots the true positive rate against the false positive rate.\n",
    "The ROC curve is visual, but we can quantify the classifier performance using the *area under the curve* (aka AUC). Ideally, AUC is 1.\n",
    "\n",
    "![ROC curve](roc-curve-v2-glassbox.png)\n",
    "\n",
    "[source: https://commons.wikimedia.org/wiki/File:Roc-draft-xkcd-style.svg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad16262-1eb7-41cc-b047-935ea5004344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,thresholds=roc_curve(y_train_5,y_score)\n",
    "plt.plot(fpr,tpr,linewidth=2);plt.grid(True)\n",
    "plt.plot([0,1],[0,1],'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebcc0e1-f953-4615-893f-b785cddf2869",
   "metadata": {},
   "source": [
    "Compare with another classifier method. We will try ***Random Forest*** and compare the two classifiers. Instead of outputing scores, RF works with probabilities. So the value returned as between 0 and 1 with the probability of appartenance to the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1e03c-84ed-4402-8ac5-169226d17f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42) # model design\n",
    "y_rf_5 = cross_val_predict(rf_clf,X_train,y_train_5,cv=3,method=\"predict_proba\")\n",
    "y_scores_rf = y_rf_5[:,1] # score in the positive class\n",
    "fpr_rf,tpr_rf,threshold_rf = roc_curve(y_train_5,y_scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a59c5-6519-478f-bdaf-808bcf217524",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_rf,tpr_rf,'r',linewidth=2)\n",
    "plt.plot(fpr,tpr,linewidth=2);plt.grid(True)\n",
    "plt.plot([0,1],[0,1],'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07968685-de62-47ad-ad9e-f507b8f257ae",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Here we will use several well known classifiers: Support Vector Machine, k-nearest neighbors, Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed564bd-be4a-45a5-b94b-245d5d23406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "clf = SVC(gamma=0.001) # model design\n",
    "clf.fit(X_train, y_train) # learn\n",
    "svc_prediction = clf.predict(X_test) # predict on test\n",
    "print(\"SVC Accuracy:\", metrics.accuracy_score(y_true=y_test ,y_pred=svc_prediction))\n",
    "\n",
    "# K-nearest Neighbors\n",
    "knn_clf = KNeighborsClassifier() # model design\n",
    "knn_clf.fit(X_train, y_train) # learn\n",
    "knn_prediction = knn_clf.predict(X_test) # predict on test\n",
    "print(\"K-nearest Neighbors Accuracy:\", metrics.accuracy_score(y_true=y_test ,y_pred=knn_prediction))\n",
    "\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier(random_state=42, verbose=True) # model design\n",
    "rf_clf.fit(X_train, y_train)# learn\n",
    "rf_prediction = rf_clf.predict(X_test) # predict on test\n",
    "print(\"Random Forest Accuracy:\", metrics.accuracy_score(y_true=y_test ,y_pred=rf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9b1b1-47be-43c8-9048-c992a205f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, prediction in zip(axes, X_test, rf_prediction):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title(f'Prediction: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc33dd-91ae-4463-aa0e-1302a165ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support Vector Machine\")\n",
    "print(f\"Classification report for classifier {clf}:\\n\"\n",
    "      f\"{metrics.classification_report(y_test, svc_prediction)}\\n\")\n",
    "disp = metrics.plot_confusion_matrix(clf, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c34db6-52e8-49bb-a692-5cb1f082bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-nearest neighbors\")\n",
    "print(f\"Classification report for classifier {knn_clf}:\\n\"\n",
    "      f\"{metrics.classification_report(y_test, knn_prediction)}\\n\")\n",
    "disp = metrics.plot_confusion_matrix(knn_clf, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ca81d-e55b-487e-9577-3ffbaa124035",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest\")\n",
    "print(f\"Classification report for classifier {rf_clf}:\\n\"\n",
    "      f\"{metrics.classification_report(y_test, rf_prediction)}\\n\")\n",
    "disp = metrics.plot_confusion_matrix(rf_clf, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8d197-950f-4863-83c2-345bf078cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score, precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7365-289b-4429-8141-b9f8ba7752cb",
   "metadata": {},
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3511fe-308a-4a29-ae5b-61caac356fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "\n",
    "y = label_binarize(y, classes=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.5, shuffle=False)\n",
    "\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(Nclasses):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:,i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "for i in range(Nclasses):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC label %1.0f (area = %0.2f)' % (i,roc_auc[i]))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61aee5c-b10a-4661-a724-4edf56b4c140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "f625eed87f201675869c1975f26c79747f846dd12cd9c70305bdb23b2c204f1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
